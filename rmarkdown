---
title: "STAT 440 Project"
author: "Victoria Jackson"
date: "April 1, 2017"
output: pdf_document
---
**Abstract** 
In mathematical statistics, it is important to understand the underlying relationships among correlated observations. One method for understanding these relationships is factor analysis. Factor analysis is a method to interpret and explore the correlations between observed variables through a smaller number of latent factors. This paper provides an introduction to Bayesian factor analysis (BFA). The key advantage of BFA over the more standard maximum likelihood factor analysis (ML-FA) is that background knowledge can be incorporated into the analysis in the form of prior distributions. This paper performs a confirmatory Bayesian factor analysis on the results of a multicultural study and compares this approach to a confirmatory ML-FA.  The paper then outlines the relative performance of a BFA as compared to ML-FA for a three-factor model. Iterated Conditional Modes (ICM) sampling  is used to estimate the parameters of the BFA model. The results of the AIC and BIC criteria reveal that **** model fits the data better than the **** model. The paper concludes by discussing further work on this topic, including removing the assumption that the variables are continuous.

**1. Introduction**
Factor analysis is a statistical method used to explain the correlation structure among observed variables in terms of a lower number of unobserved latent variables called factors. Factor models are often used in behavioural sciences, and in recent years their applications have extended to areas beyond social sciences. These models provide a convenient and flexible framework for modeling multivariate data using a smaller number of unobserved factors (Ghosh and Dunson, 2009). Exploratory factor analysis (EFA) can be used to identify the number and nature of factors that explain the variability among the data. This method is most appropriate when not much is known about the underlying factors that may explain the correlation structure in the data. On the other hand, confirmatory factor analysis (CFA) can be used when the number and nature of the underlying factors is known prior to the data analysis. Unlike EFA, CFA imposes an a priori model on the data and tests the degree of plausibility that the data were generated by that model (Heerwegh, 2014).

Maximum likelihood factor analysis (ML-FA) is usually recommended only for large samples (e.g. n greater or equal to 200) since it relies on large sample theory. For small samples, ML-FA can face issues such as model non-convergence and negative residual variances (Heerwegh, 2014). Bayesian statistics tend to perform better for smaller samples; thus, these statistics may be useful for studies that rely on small samples (Heerwegh, 2014). Consequently, Bayesian factor analysis (BFA) can be a useful method for relatively small data sets.

In the following section, we will implement Bayesian factor analysis on the data from a multicultural study (Yampolsky et al., 2016) and compare the results to ML-FA implemented on the same data. The data is based on a multicultural integration study where diverse samples of multicultural individuals completed a survey known as the Multicultural Identity Integration Scale (MULTIIS). This scale examines 3 different multicultural identity configurations: categorization, compartmentalization and integration. The scale contains 22 questions where each question belongs to one of the three identity configurations. The response for each question is an integer between 1 and 7 (1 = Not at all, 7 = Exactly). Each question contains responses from 333 multicultural individuals. We will perform confirmatory ML-FA and BFA on the data based on a three-factor model. The background knowledge about the underlying three factors in the MULTIIS data motivates the decision to include three factors in the factor analysis models (one factor for each of the multicultural identity configurations). If the model fits the data well, then the hypothesis that the scale measures the underlying factors of categorization, compartmentalization and integration

In order to build the BFA model, we will estimate the unknown factors, the factor loading matrix, and the disturbance covariance matrix. The Bayesian inference will find the posterior distribution of the estimators used in the factor analysis. The estimation will be performed using the Iterated Conditional Modes (ICM) sampling. According to Rowe and Press, ICM sampling has a number of advantages for the BFA model: 
As with Gibbs sampling, ICM sampling does not require the proportionality constant of the conditionals. However, Gibbs sampling requires the exact form apart from the constant, whereas ICM requires only the mode. The kernels of the conditionals are known distributions with modes in closed form. With ICM we must only cycle through the posterior conditional modes.

- ICM guarantees convergence even with a small number of iterations, while convergence is uncertain with Gibbs sampling. With Gibbs sampling there is always a degree of uncertainty about whether the sequence has converged and therefore uncertainty about the accuracy of the estimates (Rowe and Press, 1994).
In order to implement ICM sampling, we will find the following conditional posterior distributions:

1.	(F) given (factor loading, disturbance covariance matrix, data)

2.	(factor loadings) given (F, disturbance covariance matrix, data)

3.	(Disturbance covariance) matrix given (F, factor loadings , data)

The exact steps in the Bayesian approach will be further addressed in the Methodology section. Finally, the Discussion section will summarize our findings and suggest directions for further work.

**2. Methodology**

**Bayesian Inference**

To begin, the report will define a simple Bayesian inference model. The goal of Bayesian inference is to find the posterior distribution to estimate a parameter. The value of the posterior is proportional to the likelihood of the data (distribution of the data given the parameters), multiplied by, the prior distributions (distribution of the parameters before observation). This method is determined by Bayes rule. 
 $$ p(\theta|X) \propto p(X|\theta)\times p(\theta)$$

**2.1.Model**

The goal of factor analysis is to show that the correlation of $D$-dimensional random variable, $x$, can be expressed by $K$ latent (unobserved but inferred) factors, such that $K << D$. We want to be able to estimate the factors; these factors give insight to the structure of the data. The factor model is defined as: A $D$-dimensional observed vector is $X' = (x_1,...,x_N)$ for $N$ subjects, where each $x_i = (x_{1i},...,x_{1D})$, such that $X_{N \times D}$. $\prime$ denotes transpose of a matrix.

The Bayesian Factor model is:
$$x_i = \Lambda f_i + \epsilon_i$$
$$(D \times 1) = (D \times K) \times (K \times 1) + (D \times 1)$$ where $K < D$ for $i = 1,..,N$

In Matrix Form $$X = F \Lambda^{\prime} + \epsilon$$
$$ (N \times D) = (N \times K) \times (K \times D) + (D \times N) $$

- $\Lambda$ is the factor loading matrix of constant values. 
- $f_i$ is the factor score for ith variable, where $f_i = (f_{i1},...,f_{iK})'$ and each $f_{ij} \sim N(0,1)$ where $i = 1,...,N$ and $j = 1,...,K$, it follows $f_i \sim N(0,I_k)$ where $I_k$ is a $(k \times k)$ identity matrix. With $F^{\prime} = (f_1,...,f_N)$.
- $\epsilon_i$ is the independent residual and is normally distrubuted as $e_i \sim N(0, \Psi)$ the residuals are independent of $F$. 
- $\Psi$ is a $D \times D$ symmetric and positive semi-definite matrix. It is not assumed that $\Psi$ is diagonal, but it assumed that $E(\Psi) =$ diagonal, (diagonal on the average). 

$(F,\Lambda,\Psi)$ are all unobserved, thus we need to estimate those values. We can estimate them by finding the posterior distribution according to Bayesian Inference. 

As a result of this model, the Probability of Law is expressed as: 
$$L(x_i|f_i,\Lambda,\Psi) = N(\Lambda f_i,\Psi)$$ where $L(.)$ represents probability law, while $p(.)$ represents probability density.

In Matrix Form:
$$p(X|F,\Lambda,\Psi) = N(F \Lambda^{\prime},\Psi)$$
Thus by definition, the likelihood function for $(F,\Lambda,\Psi)$ is proportional "$\propto$" to 
$$p(X|F,\Lambda,\Psi) \propto$$ **NEED**

**Bayes Inference** 

The goal is to find the posterior distribution $p(F,\Lambda,\Psi|X)$, so that we can estimate the parameters. Where $$ p(F,\Lambda,\Psi|X) \propto p(X|F,\Lambda,\Psi)\times p(F,\Lambda,\Psi)$$
$$ \text{Posterior} \propto \text{Likelihood} \times \text{Joint Probability}$$

**2.3. Priors**

The next step is to find the priors for $(F,\Lambda,\Psi)$. Note the joint probability; $$p(F,\Lambda,\Psi) \propto p(\Lambda|\Psi)p(\Psi)p(F)$$ 
We used the same prior specifications as described in __ paper __. 
**WHY DID WE CHOOSE THE PRIORS**

1.$p(\Psi) \sim \mathcal{W}^{-1}({\mathbf B},\nu)$

$\Psi$ follows a Inverse-Wishart Distribution, this is a multivaritate version of the inverse-Gamma distribution (reference). Both $({\mathbf B},\nu)$ are hyperparameters, such that ${\mathbf B}_{(D \times D)}$ is a positive diagonal matrix and $\nu > 2D -1$ (reference). $$p(\Psi) \propto |\Psi|^{(\frac{-\nu}{2})}e^{(\frac{-1}{2})tr(\Psi^{-1}B)}$$

2.$p(\Lambda|\Psi) \sim N(\Lambda_0,H)$

We assume \Lambda|\Psi is normally distributed, a priori. We choose this prior because .$(\Lambda_0, H)$ are Hyperparameters we will assess. $H_{(K \times K)} > 0$ and is positive definite matrix and ${\Lambda_0}_{(D \times K)}$  $$p(\Lambda) \propto |\Psi|^{(\frac{-K}{2})}e^{(\frac{-1}{2})tr(\Psi^{-1}(\Lambda- \Lambda_0)H(\Lambda - \Lambda_0)^{\prime})}$$

3.$f_j \sim N(0,I_K)$
$$p(F) \propto e^{(\frac{-1}{2})tr(F^{\prime}F)}$$

Thus, we have the unassessed hyperparameters $(\Lambda_0, B, \nu, H)$, section 2.5 will discuss an empirical method of assessing these hyperparameters.

**2.4 Posterior**

Now we can combine the likelihood and the priors to create the posterior distribution.

Recall :$p(F,\Lambda,\Psi|X) \propto p(X|F,\Lambda,\Psi) \times p(\Lambda|\Psi)p(\Psi)p(F)$
We can sub in the values according to the previous specifications. 
$$p(F,\Lambda,\Psi|X) \propto |\Psi|^{(\frac{-\nu}{2})}e^{(-1/2)tr(\Psi^{-1}B)} \times 
|\Psi|^{(\frac{-K}{2})}e^{(-1/2)tr(\Psi^{-1}(\Lambda- \Lambda_0)H(\Lambda - \Lambda_0)^{\prime})}
\times e^{(\frac{-1}{2}tr(F^{\prime}F))}$$

This according to _paper_ the posterior reduces to:
$$p(F,\Lambda,\Psi|X) \propto e^{\frac{-1}{2}tr(F^{\prime} F)}|\Psi|^{\frac{N + K +\nu}{2}}e^{\frac{-1}{2}tr(\Psi^{-1}U)}$$
Where, $U = (X - F\Lambda^{\prime})^{\prime}(X- F\Lambda^{\prime}) +(\Lambda - \Lambda_0)H(\Lambda - \Lambda_0)^{\prime} + B$.Now we have the posterior distribution, from this we can estimate the parameters $(F,\Lambda,\Psi)$ but we still have the hyperparameters to assess before this estimation. 

**2.5 Assesment of Hyperparameters** $(\Lambda_0,v,B,H)$(reference = Arnold, B. C. (2008))

We will be using an empirical Bayes Method of assessing the Hyperparameteres.This method was retrieved from the book Advances in Mathematical and Statistical Modelling within Chapter 6 by B.C. Arnold in 2008. According to this source, this Empiricial Bayes Method improves upon the assessment from the orginial paper of Bayesian factor analysis, and it allows convergence to occur easier. We have four hyperparameters to assess $(\Lambda_0,\nu,B,H)$. With this method the bayesian estimate $\hat{\Lambda}$ is very close to $\Lambda_0$, this is because $\hat{\Lambda}|\Psi$ is distributed as normal with mean $\Lambda_0$. With this empirical method proposed by the book, the result was deemed a better fit due to a smaller AIC and BIC than the estimation of hyperparameters from the orginal **PS** model.  

**2.5.1 $\Lambda_0$**

$\Sigma_{D \times D}$ is the Variance-covariance matrix for $X$. $\Sigma = Var(x_i|\Lambda,\Psi) = Var(\Lambda f_i|\Lambda,\Psi) + \Psi$, because:

1. $\Lambda \perp f_i$

2. $\epsilon \sim N(0,\Psi)$, where $\Psi > 0$

3. $f_i \sim N(0,I_K)$

4. $x_i \perp x_j$ $\forall$  $i \neq j, i = 1,...,j,...,N$

Thus $Var(\Lambda f_i|\Lambda,\Psi) + \Psi = \Lambda[Var(f_i|\Lambda,\Psi)]\Lambda\prime + \Psi$
$= \Lambda\Lambda\prime + \Psi$ 

Thus $\Sigma =  \Lambda\Lambda\prime + \Psi, \Psi > 0$ 

Now, $\Sigma$ is positive and semi-definite, we can eigen decompose $\Sigma$,
$\Sigma = \Gamma D_{\theta}\Gamma\prime$

1. $\Gamma\Gamma\prime = I_{D \times D}$

2. $D_{\theta} = diag(\lambda_1,...,\lambda_D)$ such that $\lambda_1 \ge \lambda_2 \ge ...\ge \lambda_D > 0$

3. Now only keep the K largest $\lambda$ and corresponding vectors

4. Now redefine:
$\bar{D}_{K \times K} = diag(\lambda_1,...,\lambda_K)$, $\bar{\Gamma}_{D \times K}$ which is the matrix of orthogonal columns of latent vectors, corresponding to the K largest $\lambda$s

5. Define ${\Lambda_0}_{D \times K} = \bar{\Gamma}_{D \times K}\bar{D}_{K \times K}^{1/2}$

6. $\Sigma \approx \Lambda_0\Lambda_0\prime$

The estimate for $\Lambda_0$ is $\bar{\Gamma}_{D \times K}\bar{D}_{K \times K}^{1/2}$

**2.5.2 $\nu$**

$\nu$ is unknown, but we do know it was used to create the prior for $\Psi$. Thus $E(\Psi) = \frac{B}{v - 2*K - 2}$
We can find the $\nu$ that minimizes $E(\Psi)$, thus $\nu > 2*K + 2$ and $\nu \sim 2*K + 3$.

**2.5.3 $B$**

$B$ is also unknown. $E(\Psi) = \frac{B}{v - 2*K - 2}$, $B$ is diagonal, since $v = 2*K + 3$. This means $E(\Psi) = B$.
Recall,

1. $\Psi$ > 0 and $\Lambda\prime\Lambda + \Psi = \Sigma$ 

2. $\Psi \approx \Psi* = \Lambda_0\Lambda_0\prime$. Thus $E(\Psi) \approx E(\Psi^{*}) = \Psi*$. This implies $\Psi^{*} = B$ 

3. $B$ is diagonal thus $\Psi_{ii} = b_{ii}$ and $b_{ij} = 0, \forall i\neq j$

Thus we have estimated $B$ to be the diagonal elements of $\Psi^{*}$

**2.5.4 $H$**

We can covert $\Lambda$ to be represented as a series of vectors $\lambda = Vec(\Lambda) = (\lambda_1^{\prime},...,\lambda_K^{\prime})^{\prime}$.Then, $(\lambda|\Psi) \sim N(\lambda, H^{-1}\Psi)$ Which means,

1. $Var(\lambda|\Psi) = H^{-1}\Psi$

2. $Var(\lambda) = H^{-1}E(\Psi)$

3. $Cov(\lambda_i,\lambda_j|\Psi) = H_{ij}^{-1}\Psi$

4. Now we want to find ${H_{ij}}^{-1}I_{D \times D} = Cov[(\Psi^{\frac{1}{2}}\lambda_i,\Psi^{\frac{1}{2}}\lambda_j)|\Psi) = Cov[(\Psi^{\frac{1}{2}}\lambda_i,\Psi^{\frac{1}{2}}\lambda_j)] = H_{ij}^{-1}I_{D \times D}$

$\Delta^* = (\Psi*^{1/2}\lambda_0)^{\prime} = (\delta_i^*,...,\delta_D^*)$, $H^{-1} = Var(\delta_i) = \frac{1}{D} sum_{i=1}^{D}{(\delta_i^* - \bar{\delta^*})(\delta_i^* - \bar{\delta^*})^{\prime}}$, where $\bar{\delta^*} = \sum_{i=1}^{D}(\delta_i^*)$

**2.6 Iterative Convergence of Hyperparameters**

From the theory in section 2.5, Advances in Mathematical and Statistical Modelling also proposed a method of convergence to estimate the hyperparameters. 

Step 1. Calculate $\Sigma$ as the Variance covariance matrix of X, denote as $\Sigma^{(1)}$ from that use the formula to calculate $\Lambda_0$, refer to that as $\Lambda_0^{(1)}$. Now calcuate $\Psi^{(1)}$ $= \Sigma^{(1)} - \Lambda_0^{(1)} {\Lambda_0^{(1)}}^{\prime}$.

Step 2. Now, $\Psi^{(2)} = diag(\Psi^{(1)})$, use this to find ${\Sigma}^{(2)} = {\Sigma}^{(1)} - {\Psi}^{(2)}$. Retrieve $\Lambda_0^{(2)}$ from $\Sigma^{(2)} = \Lambda_0^{(2)}{\Lambda_0^{(2)}}^{\prime}$.

Step 3. Calculate the difference between the values, ${dif_\lambda} = \Lambda_0^{(1)} - \Lambda_0^{(2)}$, and  ${dif_\Psi}= \Psi^{(1)} - \Psi^{(2)}$, ${dif_\lambda}^{2} = {dif_\lambda}{dif_\lambda}^{\prime}$, ${dif_\Psi}^{2} = {dif_\Psi}{dif_\Psi}^{\prime}$

Step 4. Repeat until convergence, where the trace or determinat of ${dif_\lambda}^{2}$ and ${dif_\Psi}^{2}$ are less than some desired threshold.

Step 5. Take the estimates as the converged values ${\Lambda_0}^{(i)}$, ${\Psi}^{(i)}$, but only take the postive diagonal elements of ${\Psi}^{(i)}$, call this $\Psi^{*}$

Step 6. Use the formulas of section 2.5 to retrieve the values of $(B,\nu,H)$

Now that we have the assessed hyperparameters, we can begin finding the conditinal posterior distributions in order to retrieve estimates for the parameters $(F,\Lambda,\Psi)$. 

**2.7 Conditional Posterior Distributions**

Now we can find the conditional posteriors distrubtions for the parameters $(F, \Lambda, \Psi)$, these distributions have been retrieved from **paper**. The conditional distribution will allow us to perform Gibbs Sampling.

**2.7.1 $\Lambda$**
$$p(\Lambda|F,\Psi,X) \propto e^{(\frac{-1}{2})tr(\Psi^{-1}[(\Lambda -\tilde{\Lambda})(H + F^{\prime}F)(\Lambda -\tilde{\Lambda})^{\prime}])}$$
where $\tilde{\Lambda} = (X^{\prime}F + \Lambda_0H)(H + F^{\prime}F)^{-1}$

**2.7.2 $\Psi$**
$$p(\Psi|F,\Lambda,X) = |\Psi|^{\frac{(N + K -\nu)}{2}}e^{\frac{-1}{2}tr(\Psi^{-1}U)}$$
where $U = (X - F\Lambda^{\prime})^{\prime}(X- F\Lambda^{\prime}) +(\Lambda - \Lambda_0)H(\Lambda - \Lambda_0)^{\prime} + B$

**2.7.3 F**
$$p(F|\Lambda,\Psi,X) = e^{(\frac{-1}{2})tr[(F - \tilde{F})(I_K + \Lambda^{\prime}\Psi^{-1}\Lambda)(F - \tilde{F})^{\prime}])}$$
where $\tilde{F} = X\Psi^{-1}\Lambda(I_K + \Lambda^{\prime}\Psi\Lambda)^{-1}$

**2.8 Marginal Posterior Distributions**
In order to get an estimation for F, we need to find the marginal distribution $p(F|X)$. First we integrate the posterior distribution $p(F,\Lambda,\Psi|X)$ with respect to $\Psi$. The conditional distribution are retrieved from **Paper**

1. $p(F, \Lambda| X) = p(F)|U|^{-\frac{N+K+\nu-D-1}{2}}$ 
where $U = (X - F\Lambda^{\prime})^{\prime}(X- F\Lambda^{\prime}) +(\Lambda - \Lambda_0)H(\Lambda - \Lambda_0)^{\prime }+ B$

2. Now integrate again with respect to \Lambda to get the marginal posterior distribution $p(F|X)$, according to the **paper**.

$$p(F|X) \propto \frac{p(F){|H + F^{\prime}F|}^{{N - \nu -2*K - D}/2}}{{|A+ (F-\hat{F})^{\prime}(I_N - X W^{-1} X^{\prime})(F- F^{\prime})|}^{(N -\nu - D -1)/2}}$$

Where $\hat{F} = (I_N − X(X^{\prime}X −W)^{−1}X^{\prime})XW^{−1}\Lambda_0 H$
Since $p(F|X)$ appears to approximately follow a T-distribution, this implies for large samples the estimate $F$, the factor score matrix is $\hat{F}$ (reference Book)

where $W = X^{\prime}X + B + \Lambda_0 H \Lambda_0^{\prime}$,

Where $A = H - (\Lambda_0H)^{\prime}W^{-1}\Lambda_0H -(XW^{-1}\Lambda_0H)^{\prime}(I_N - X(X^{\prime}X-W)^{-1}X^{\prime})(XW^{-1}\Lambda_0H)$

**2.9 Estimates**
This report will use three methods of estimating the parameters $(F, \Lambda, \Psi)$ using Bayesian Factor Analysis. First, the mean estimates from the Bayesian factor analysis model. Second, the gibbs sampler, and third, the ICM method.

**TO DO: Justify choices of estimators, what are the benefits and drawbacks of each**

**2.9.1 Bayesian Estimates**
The Bayesian estimators are retrieved from -paper-. This method utilized the marginal posterior of the parameters to create an estimate. This estimate utilizes the expectation properties of each function. These are the results;
Recall;

Estimator $\hat{F} = (I_N − X(X^{\prime}X −W)^{−1}X^{\prime})XW^{−1}\Lambda_0 H$

The marginal distribution from section 2.6 $p(\Lambda,F|X)$ combined with $p(F)$ creates the conditional distribution $Lambda|F$, according to (book), this forms another T-distribution, this we can estimate $\Lambda$, $\hat{\Lambda} = E(\Lambda|\hat{F},X)$. 
Estimator $\hat{\Lambda} = (X^{\prime}\hat{F} + \Lambda_0H)(H + \hat{F}^{\prime}\hat{F})^{-1}$

To estimate, $\Psi$, according to (book), we can use the joint posterior $(\Lambda,F,\Psi)|X$, the marginal posterior $\Lambda,F|X$, the conditional distibution $\Psi|F,\Lambda, X$, is the kernel of the inverted Wishart Distribution:
$$p(\Psi|\hat{\Lambda},\hat{F},X) = \frac{e^{(\frac{1}{2})tr(\Psi^{1}\hat{G})}}{|\Psi|^{(N + DK - \nu)/2}}$$
where $\hat{G} = (X - \hat{F}\hat{\Lambda}^{\prime})^{\prime}(X- \hat{F}\hat{\Lambda}^{\prime}) +(\hat{\Lambda} - \Lambda_0)H(\hat{\Lambda} - \Lambda_0)^{\prime} + B$
Thu using the property of inverted Wishart Distribution, the expectation is (reference)
Estimator $\hat{\Psi} = \frac{\hat{G}}{(N + DK + \nu − 2p − 2)}$

**2.9.2 Iterated Conditional Models Sampler**
Iterated conditional model (ICM)(Lindley and Smith 1972), is a deterministic method. This method in relation the factor analysis was retrieved from -paper-. ICM, attempts to find the maximum of the posterior distribution by differentiating with respect to each variable $(F,\Lambda, \Psi)$, we have to retrieve the posterior conditional distributions, then differentiate to obtain the maximum estimation for each variable. Then we apply an optimatization function to find the estimates of the variables $(F,\Lambda, \Psi)$ until convergence. This optimization entails;

We start with and initial value $\tilde{F}_{(0)}$ this value we will select to be the mean estimate from 2.7.1.

$\tilde{\Lambda}_{(i+1)} = (X^{\prime}\tilde{F}_{(i)} + \Lambda_0H)(H +{\tilde{F}_{(i)}}^{\prime}\tilde{F}_{(i)})^{-1}$

$\tilde{\Psi}_{(i+1)} = \frac{(X - \tilde{F}_{(i)}{\tilde{\Lambda}^{\prime}_{(i+1)}})^{\prime}(X - \tilde{F}_{(i)}{\tilde{\Lambda}^{\prime}_{(i+1)}}) + ({\tilde{\Lambda}_{(i+1)}} - \Lambda_0)H({\tilde{\Lambda}_{(i+1)}} - \Lambda_0)^{\prime} + B}{N + K + \nu}$

$\tilde{F}_{(i+1)} = X\tilde{\Psi}_{(i+1)}\tilde{\Lambda}_{(i+1)}(I_K + \tilde{\Lambda}^{\prime}_{(i+1)}\tilde{\Psi}_{(i+1)}{\tilde{\Lambda}_{(i+1)}})^{-1}$

Repeat until Convergence is obtained. 
** WHY IS ICM A GOOD CHOICE**

**2.10 Frequentist Approach to Factor Analysis**
Beyond Bayesian Factor analysis, there also exist a frequentist approach. This method entails finding the values of $(F,\Lambda,\Psi)$ by maximizing the log likelihood function of the factor analysis model. The results of this will be estimated within the function $`r{factanal}`$. 
** FREQUENTIST VS BAYESIAN IN THEORY**

**3. RESULTS**
MULTIIS Data Set explain

3.1 Choice of Factors
 3 vs 4 vs 6 
 scree and kaiser and 3 groups
3.2 factanal results

3.3 Frequentist results

3.4 Bayesian results
3.4.1 Mean Estimates
3.4.2 Gibbs Estimates

3.5 Compare Results

3.6 Interpret factors

**4. Discussion**

**5. Conclusion**

**6.Appendix**
code (Test approaches)

**7. References**



